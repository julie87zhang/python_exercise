{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity_A_B_testing \n",
    "\n",
    "#### Context\n",
    "\n",
    "At the time of this experiment, Udacity courses currently have two options on the course overview page: \"start free trial\", and \"access course materials\". If the student clicks \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.\n",
    "In the experiment, Udacity tested a change where if the student clicked \"start free trial\", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead.\n",
    "\n",
    "#### Hypothesis\n",
    "The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough timeâ€”without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "\n",
    "#### Unit of diversion \n",
    "The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading experiment data\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_pd=pd.read_csv('control_group.csv')\n",
    "exp_pd=pd.read_csv('experiment_group.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37, 5), (37, 5))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_pd.shape,exp_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date           37\n",
       "Pageviews      37\n",
       "Clicks         37\n",
       "Enrollments    23\n",
       "Payments       23\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_pd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Pageviews</th>\n",
       "      <th>Clicks</th>\n",
       "      <th>Enrollments</th>\n",
       "      <th>Payments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat, Oct 11</td>\n",
       "      <td>7723</td>\n",
       "      <td>687</td>\n",
       "      <td>134.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sun, Oct 12</td>\n",
       "      <td>9102</td>\n",
       "      <td>779</td>\n",
       "      <td>147.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon, Oct 13</td>\n",
       "      <td>10511</td>\n",
       "      <td>909</td>\n",
       "      <td>167.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue, Oct 14</td>\n",
       "      <td>9871</td>\n",
       "      <td>836</td>\n",
       "      <td>156.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed, Oct 15</td>\n",
       "      <td>10014</td>\n",
       "      <td>837</td>\n",
       "      <td>163.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Pageviews  Clicks  Enrollments  Payments\n",
       "0  Sat, Oct 11       7723     687        134.0      70.0\n",
       "1  Sun, Oct 12       9102     779        147.0      70.0\n",
       "2  Mon, Oct 13      10511     909        167.0      95.0\n",
       "3  Tue, Oct 14       9871     836        156.0     105.0\n",
       "4  Wed, Oct 15      10014     837        163.0      64.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date           37\n",
       "Pageviews      37\n",
       "Clicks         37\n",
       "Enrollments    23\n",
       "Payments       23\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_pd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Pageviews</th>\n",
       "      <th>Clicks</th>\n",
       "      <th>Enrollments</th>\n",
       "      <th>Payments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat, Oct 11</td>\n",
       "      <td>7716</td>\n",
       "      <td>686</td>\n",
       "      <td>105.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sun, Oct 12</td>\n",
       "      <td>9288</td>\n",
       "      <td>785</td>\n",
       "      <td>116.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon, Oct 13</td>\n",
       "      <td>10480</td>\n",
       "      <td>884</td>\n",
       "      <td>145.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue, Oct 14</td>\n",
       "      <td>9867</td>\n",
       "      <td>827</td>\n",
       "      <td>138.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed, Oct 15</td>\n",
       "      <td>9793</td>\n",
       "      <td>832</td>\n",
       "      <td>140.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Pageviews  Clicks  Enrollments  Payments\n",
       "0  Sat, Oct 11       7716     686        105.0      34.0\n",
       "1  Sun, Oct 12       9288     785        116.0      91.0\n",
       "2  Mon, Oct 13      10480     884        145.0      79.0\n",
       "3  Tue, Oct 14       9867     827        138.0      92.0\n",
       "4  Wed, Oct 15       9793     832        140.0      94.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345543, 344660, 690203)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check sample size\n",
    "sample_size_control = control_pd.Pageviews.sum()\n",
    "sample_size_experiment = exp_pd.Pageviews.sum()\n",
    "sample_size=sample_size_control+sample_size_experiment\n",
    "print('control group size is',sample_size_control)\n",
    "print('experiment group size is',sample_size_experiment)\n",
    "print('total size of 2 groups',sample_size)\n",
    "#sample_size_control,sample_size_experiment,sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following metrics would you choose to measure for this experiment and why? For each metric you choose, indicate whether you would use it as an invariant metric or an evaluation metric. The practical significance boundary for each metric, that is, the difference that would have to be observed before that was a meaningful change for the business, is given in parentheses. All practical significance boundaries are given as absolute changes.\n",
    "Any place \"unique cookies\" are mentioned, the uniqueness is determined by day. (That is, the same cookie visiting on different days would be counted twice.) User-ids are automatically unique since the site does not allow the same user-id to enroll twice.\n",
    "\n",
    "* Number of cookies: That is, number of unique cookies to view the course overview page. (dmin=3000)\n",
    "* Number of user-ids: That is, number of users who enroll in the free trial. (dmin=50)\n",
    "* Number of clicks: That is, number of unique cookies to click the \"Start free trial\" button (which happens before the free trial screener is trigger). (dmin=240)\n",
    "* Click-through-probability: That is, number of unique cookies to click the \"Start free trial\" button divided by number of unique cookies to view the course overview page. (dmin=0.01)\n",
    "* Gross conversion: That is, number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the \"Start free trial\" button. (dmin= 0.01)\n",
    "* Retention: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. (dmin=0.01)\n",
    "* Net conversion: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the \"Start free trial\" button. (dmin= 0.0075)\n",
    "\n",
    "You should also decide now what results you will be looking for in order to launch the experiment. Would a change in any one of your evaluation metrics be sufficient? Would you want to see multiple metrics all move or not move at the same time in order to launch? This decision will inform your choices while designing the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invariant metric  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invariant Metrics don't change in the beginning of the experiment, and it still shouldn't change after the experiment.\n",
    "\n",
    "This experiment will potentially change user behaviours after they click \"Start free trail\" button then will be exposed to the new pop up window to fill out the hours of study before proceeding to checkout as before. Therefore all the metrics at and before button clicking should be not changed after the experiement. \n",
    "\n",
    "Based on this reasonings, we can choose those metrics as invariant metrics for us to do the sanity check later on.\n",
    "\n",
    "* Number of cookies: That is, number of unique cookies to view the course overview page. (dmin=3000)\n",
    "* Number of clicks: That is, number of unique cookies to click the \"Start free trial\" button (which happens before the free trial screener is trigger). (dmin=240)\n",
    "* Click-through-probability: That is, number of unique cookies to click the \"Start free trial\" button divided by number of unique cookies to view the course overview page. (dmin=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the invariant metrics, we expect the following metrics to be affected by the treatment and vary between control and treatment group.\n",
    "\n",
    "* Gross conversion: That is, number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the \"Start free trial\" button. (dmin= 0.01)\n",
    "* Retention: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. (dmin=0.01)\n",
    "* Net conversion: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the \"Start free trial\" button. (dmin= 0.0075)\n",
    "\n",
    "Lastly, we would also expect the number of user-ids (i.e. the number of users who enroll in the free trial; dmin=-50) to decrease. However, the metric is not normalized and would not provide any information we are not already capturing with gross conversion (as the number of clicks will be controlled for). Thus, we will not use it as an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses \n",
    "Given the available and selected metrics, we can now specify our hypotheses. While it could be argued that in some cases a one-sided test is appropriate, we are thereby sticking with a more conservative two-sided test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hypothese 1*\n",
    "* H0 : GC treatment = GC control\n",
    "* H1 : GC treatment != GC control\n",
    "\n",
    "*Hypothese 2*\n",
    "* H0 : R treatment = R control\n",
    "* H1 : R treatment != R control\n",
    "\n",
    "*Hypothese 3*\n",
    "* H0 : CN treatment = CN control\n",
    "* H1 : CN treatment != CN control\n",
    "\n",
    "Note: later on, we will drop the second hypothesis as it would demand a sample size that requires the the test to run unreasonably long "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring variability in metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all selected evaluation metrics, making analytic esitmate of its standard deviation, given a smaple size of 5000 cookies visiting the course overview page and baseline values https://docs.google.com/spreadsheets/d/1MYNUtC47Pg8hdoCjOXaHqF-thheGpUshrFA21BAJnNc/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric Name</th>\n",
       "      <th>Estimator</th>\n",
       "      <th>dmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>Cookies</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>3000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL</th>\n",
       "      <td>Clicks</td>\n",
       "      <td>3200.000000</td>\n",
       "      <td>240.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>User-ids</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>-50.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTP</th>\n",
       "      <td>Click-through-probability</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CG</th>\n",
       "      <td>Gross conversion</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>-0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>Retention</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN</th>\n",
       "      <td>Net conversion</td>\n",
       "      <td>0.109313</td>\n",
       "      <td>0.0075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric Name     Estimator       dmin\n",
       "C                      Cookies  40000.000000  3000.0000\n",
       "CL                      Clicks   3200.000000   240.0000\n",
       "ID                    User-ids    660.000000   -50.0000\n",
       "CTP  Click-through-probability      0.080000     0.0100\n",
       "CG            Gross conversion      0.206250    -0.0100\n",
       "R                    Retention      0.530000     0.0100\n",
       "CN              Net conversion      0.109313     0.0075"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Storing baseline data\n",
    "d = {\"Metric Name\": [\"Cookies\", \"Clicks\", \"User-ids\", \"Click-through-probability\", \"Gross conversion\", \"Retention\", \"Net conversion\"], \n",
    "     \"Estimator\": [40000, 3200, 660, 0.08, 0.20625, 0.53, 0.109313],\n",
    "     \"dmin\": [3000, 240, -50, 0.01, -0.01, 0.01, 0.0075]}\n",
    "md = pd.DataFrame(data=d, index=[\"C\", \"CL\", \"ID\", \"CTP\", \"CG\", \"R\", \"CN\"])\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating standard errors\n",
    "Next, we need to calculate the standard deviation of the sampling distribution of the sample mean (standard error, in short) for each of the evaluation metrics. To be more precise, in this case we calculate the estimated standard errors of the sample proportions as our evaluation metrics are probabilities. The standard error is an estimate of how far the sample proportion is likely to be from the population proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "Since the unit of diversion is the same as the unit of analysis (denominator of the metric formula) for each evaluation metric (cookie in the case of Gross Conversion and Net Conversion and user-id in the case of Retention) and we can make assumptions about the distributions of the metrics (binominal), we can calculate the standard errors analytically (instead of empirically).\n",
    "\n",
    "Further, as n is relatively large in each case, we can assume that the sampling distribution of a sample proportion approaches a normal distribution (due to the Central Limit Theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing standard errors\n",
    "Given above assumption, we can approximate SE using square root of (p_hat * (1 - p_hat )/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column to store standard errors\n",
    "md['SE']=np.nan\n",
    "# formula to calculate SE\n",
    "def StandardError (n,p):\n",
    "    return (p*(1-p)/n)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating standard errors for evaluation metrics and store them in md\n",
    "for i in [\"CG\", \"CN\"]:\n",
    "    md.loc[i,'SE']=StandardError(md.loc['CL','Estimator'],md.loc[i,'Estimator'])    \n",
    "md.loc['R','SE']=StandardError(md.loc['ID','Estimator'],md.loc['R','Estimator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric Name</th>\n",
       "      <th>Estimator</th>\n",
       "      <th>dmin</th>\n",
       "      <th>SE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>Cookies</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>3000.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL</th>\n",
       "      <td>Clicks</td>\n",
       "      <td>3200.000000</td>\n",
       "      <td>240.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>User-ids</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>-50.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTP</th>\n",
       "      <td>Click-through-probability</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CG</th>\n",
       "      <td>Gross conversion</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>-0.0100</td>\n",
       "      <td>0.007153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>Retention</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.019427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN</th>\n",
       "      <td>Net conversion</td>\n",
       "      <td>0.109313</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.005516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric Name     Estimator       dmin        SE\n",
       "C                      Cookies  40000.000000  3000.0000       NaN\n",
       "CL                      Clicks   3200.000000   240.0000       NaN\n",
       "ID                    User-ids    660.000000   -50.0000       NaN\n",
       "CTP  Click-through-probability      0.080000     0.0100       NaN\n",
       "CG            Gross conversion      0.206250    -0.0100  0.007153\n",
       "R                    Retention      0.530000     0.0100  0.019427\n",
       "CN              Net conversion      0.109313     0.0075  0.005516"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.05, 'beta': 0.2}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#storing alpha and beta in a dictionary\n",
    "error_prob = {\"alpha\": 0.05, \"beta\": 0.20}\n",
    "error_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining experiment sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric Name</th>\n",
       "      <th>Estimator</th>\n",
       "      <th>dmin</th>\n",
       "      <th>SE</th>\n",
       "      <th>n_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>Cookies</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>3000.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL</th>\n",
       "      <td>Clicks</td>\n",
       "      <td>3200.000000</td>\n",
       "      <td>240.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>User-ids</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>-50.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTP</th>\n",
       "      <td>Click-through-probability</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CG</th>\n",
       "      <td>Gross conversion</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>-0.0100</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>638940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>Retention</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.019427</td>\n",
       "      <td>4737771.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN</th>\n",
       "      <td>Net conversion</td>\n",
       "      <td>0.109313</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>685336.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric Name     Estimator       dmin        SE        n_C\n",
       "C                      Cookies  40000.000000  3000.0000       NaN        NaN\n",
       "CL                      Clicks   3200.000000   240.0000       NaN        NaN\n",
       "ID                    User-ids    660.000000   -50.0000       NaN        NaN\n",
       "CTP  Click-through-probability      0.080000     0.0100       NaN        NaN\n",
       "CG            Gross conversion      0.206250    -0.0100  0.007153   638940.0\n",
       "R                    Retention      0.530000     0.0100  0.019427  4737771.0\n",
       "CN              Net conversion      0.109313     0.0075  0.005516   685336.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create new column n_c to store sample sizes\n",
    "md[\"n_C\"] = np.nan\n",
    "\n",
    "#define function for approach B\n",
    "def get_sampleSize (alpha, beta, p, dmin):\n",
    "    '''Return sample size given alpha, beta, p and dmin'''\n",
    "    return (pow((norm.ppf(1-alpha/2)*(2*p*(1-p))**0.5+norm.ppf(1-beta)*(p*(1-p)+(p+dmin)*(1-(p+dmin)))**0.5),2))/(pow(dmin,2))\n",
    "\n",
    "#calculate sample sizes for evaluation metrics with defined adjustments and store results in md\n",
    "for i in [\"CG\", \"CN\"]:\n",
    "    md.at[i, \"n_C\"] = round((get_sampleSize(error_prob[\"alpha\"], error_prob[\"beta\"], md.loc[i][\"Estimator\"], md.loc[i][\"dmin\"])/md.loc[\"CTP\"][\"Estimator\"])*2)\n",
    "\n",
    "md.at[\"R\", \"n_C\"] = round(((get_sampleSize(error_prob[\"alpha\"], error_prob[\"beta\"], md.loc[\"R\"][\"Estimator\"], md.loc[\"R\"][\"dmin\"])/md.loc[\"CTP\"][\"Estimator\"])/md.loc[\"CG\"][\"Estimator\"])*2)\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our calculations, we would need around 638,940 pageviews (cookies) to test the first hypothesis (given our assumptions on alpha, beta, baseline conversions and dmin). To additionally test the third hypothesis, we would need a total of 685,336 pageviews. And, in case we would like to also test the second hypothesis, we would need a total of around 4,737,771 pageviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment exposure and duration\n",
    "Now, for each case, we can calculate how many days we would approximately need to run the experiment in order to reach n_C. According to the challenge description, we are thereby assuming that there are no other experiments we want to run simultaneously. So, theoretically, we could divert 100% of the traffic to our experiment (i.e. about 50% of all visitors would then be in the treatment condition). Given our estimation that there are about 40,000 unique pageviews per day, this would result in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days required for CG : 15.97\n",
      "Days required for CG+CN : 17.13\n",
      "Days required for CG+CN+R : 118.44\n"
     ]
    }
   ],
   "source": [
    "#traffic diverted to experiment [0:1]\n",
    "traffic_diverted = 1\n",
    "\n",
    "#Days it would take to run experiment for each case\n",
    "for i, j in zip([\"CG\", \"CN\", \"R\"],[\"CG\", \"CG+CN\", \"CG+CN+R\"]):\n",
    "   print(\"Days required for\",j,\":\", round(md.loc[i][\"n_C\"]/(md.loc[\"C\"][\"Estimator\"]*traffic_diverted),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we would need to run the experiment for about 119 days in order to test all three hypotheses (and this does not even take into account the 14 additional days (free trial period) we have to wait until we can evaluate the experiment). Such a duration (esp. with 100% traffic diverted to it) appears to be very risky. First, we cannot perfom any other experiment during this period (opportunity costs). Secondly, if the treatment harms the user experience (frustrated students, inefficient coaching resources) and decreases conversion rates, we won't notice it (or cannot really say so) for more than four months (business risk). Consequently, it seems more reasonable to only test the first and third hypothesis and to discard retention as an evaluation metric. Especially since net conversion is a product of rentention and gross conversion, so that we might be able to draw inferences about the retention rate from the two remaining evaluation metrics.\n",
    "\n",
    "So, how much traffic should we divert to the experiment? Given the considerations above, we want the experiment to run relatively fast and for not more than a few weeks. Also, as the nature of the experiment itself does not seem to be very risky (e.g. the treatment doesn't involve a feature that is critical with regards to potential media coverage), we can be confident in diverting a high percentage of traffic to the experiment. Still, since there is always the potential that something goes wrong during implemention, we may not want to divert all of our traffic to it. Hence, 80% (22 days) would seem to be quite reasonable. However, when we look at the data provided by Udacity (see 4.1) we see that it takes 37 days to collect 690,203 pageviews, meaning that they most likely diverted somewhere between 45% and 50% of their traffic to the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment duration in days, CN+CG:  36.45\n"
     ]
    }
   ],
   "source": [
    "#traffic diverted to experiment\n",
    "traffic_diverted = 0.47\n",
    "\n",
    "#Days it would take to run experiment if we use net conversion and gross coversion as evaluation metrics\n",
    "print(\"Experiment duration in days, CN+CG: \",round(md.loc[\"CN\"][\"n_C\"]/(md.loc[\"C\"][\"Estimator\"]*traffic_diverted),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Invariants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by checking whether your invariant metrics are equivalent between the two groups. If the invariant metric is a simple count that should be randomly split between the 2 groups, you can use a binomial test as demonstrated in Lesson 5. Otherwise, you will need to construct a confidence interval for a difference in proportions using a similar strategy as in Lesson 1, then check whether the difference between group values falls within that confidence level.\n",
    "If your sanity checks fail, look at the day by day data and see if you can offer any insight into what is causing the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CI_left</th>\n",
       "      <th>CI_right</th>\n",
       "      <th>obs</th>\n",
       "      <th>passed?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.49882</td>\n",
       "      <td>0.50118</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL</th>\n",
       "      <td>0.495885</td>\n",
       "      <td>0.504115</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTP</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CI_left  CI_right     obs passed?\n",
       "C     0.49882   0.50118  0.5006     yes\n",
       "CL   0.495885  0.504115  0.5005     yes\n",
       "CTP       NaN       NaN     NaN     NaN"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create empty dataframe to store sanity check results\n",
    "sanity_check = pd.DataFrame(columns=[\"CI_left\", \"CI_right\", \"obs\",\"passed?\"], index=[\"C\", \"CL\", \"CTP\"])\n",
    "\n",
    "#set alpha and p_hat\n",
    "p = 0.5\n",
    "alpha = 0.05\n",
    "\n",
    "#fill dataframe with results from binomial test\n",
    "#for cookies and clicks do the following\n",
    "for i,j in zip([\"C\", \"CL\"], [\"Pageviews\", \"Clicks\"]):\n",
    "    #calculate the number of successes (n_control) and number of observations (n)\n",
    "    n = control_pd[j].sum()+exp_pd[j].sum()\n",
    "    n_control = control_pd[j].sum()\n",
    "    \n",
    "    #compute confidence interval\n",
    "    sanity_check.at[i, \"CI_left\"] = p-(norm.ppf(1-alpha/2)*StandardError(n,p))\n",
    "    sanity_check.at[i, \"CI_right\"] = p+(norm.ppf(1-alpha/2)*StandardError(n,p))\n",
    "    \n",
    "    #compute observed fraction of successes\n",
    "    sanity_check.at[i, \"obs\"] = round(n_control/(n),4)\n",
    "    \n",
    "    #check if the observed fraction of successes lies within the 95% confidence interval\n",
    "    if sanity_check.at[i, \"CI_left\"] <= sanity_check.at[i, \"obs\"] <= sanity_check.at[i, \"CI_right\"]:\n",
    "        sanity_check.at[i, \"passed?\"] = \"yes\"\n",
    "    else:\n",
    "        sanity_check.at[i, \"passed?\"] = \"no\"\n",
    "\n",
    "#return results\n",
    "sanity_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CI_left</th>\n",
       "      <th>CI_right</th>\n",
       "      <th>obs</th>\n",
       "      <th>passed?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.49882</td>\n",
       "      <td>0.50118</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL</th>\n",
       "      <td>0.495885</td>\n",
       "      <td>0.504115</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTP</th>\n",
       "      <td>-0.00129566</td>\n",
       "      <td>0.00129566</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CI_left    CI_right     obs passed?\n",
       "C       0.49882     0.50118  0.5006     yes\n",
       "CL     0.495885    0.504115  0.5005     yes\n",
       "CTP -0.00129566  0.00129566  0.0001     yes"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute CTP for both groups\n",
    "CTP_control = control_pd[\"Clicks\"].sum()/control_pd[\"Pageviews\"].sum()\n",
    "CTP_experiment = exp_pd[\"Clicks\"].sum()/exp_pd[\"Pageviews\"].sum()\n",
    "\n",
    "#compute sample standard deviations for both groups\n",
    "S_control = (CTP_control*(1-CTP_control))**0.5\n",
    "S_experiment = (CTP_experiment*(1-CTP_experiment))**0.5\n",
    "\n",
    "#compute SE_pooled\n",
    "SE_pooled = (S_control**2/control_pd[\"Pageviews\"].sum()+S_experiment**2/exp_pd[\"Pageviews\"].sum())**0.5\n",
    "\n",
    "#compute 95% confidence interval and store it in sanity check\n",
    "alpha = 0.05\n",
    "\n",
    "sanity_check.at[\"CTP\", \"CI_left\"] = 0-(norm.ppf(1-alpha/2)*SE_pooled)\n",
    "sanity_check.at[\"CTP\", \"CI_right\"] = 0+(norm.ppf(1-alpha/2)*SE_pooled)\n",
    "\n",
    "#compute observed difference d and store it in sanity check\n",
    "sanity_check.at[\"CTP\", \"obs\"] = round(CTP_experiment - CTP_control,4)\n",
    "\n",
    "#check if sanity check is passed\n",
    "if sanity_check.at[\"CTP\", \"CI_left\"] <= sanity_check.at[\"CTP\", \"obs\"] <= sanity_check.at[\"CTP\", \"CI_right\"]:\n",
    "    sanity_check.at[\"CTP\", \"passed?\"] = \"yes\"\n",
    "else:\n",
    "    sanity_check.at[\"CTP\", \"passed?\"] = \"no\"\n",
    "\n",
    "#return results\n",
    "sanity_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our pool proportion is still within the interval, this does pass sanity check. Then we don't have to worry about checking further number of cookies by day data.\n",
    "\n",
    "In summary, if you don't pass the sanity check, you should not continue to analyse your experiment. Commonly there's three things that you could do. First one is in the technical level. You should take to your engineers what went wrong, atribute that is really different in invariant metrics. Secondly, you could do retrospective analysis in your data, see if you can debug through slicing by features that we have talked earlier. Finally you could check the metric by pre-period and experiment period that's been discussed in previous blog. If you see changes in both period, then it maybe backends/infrastructure failure. If you see changes in experiment, then it could means there's something wrong in your experiment.\n",
    "\n",
    "There's many thing that you can investigate when something is wrong between experiment and control groups. You could have different data capture between both groups, or you could have different filtering. Or users have reset their cookies, so you can check all of these things.\n",
    "\n",
    "Learning effect could be attribute to changes in both your experiment and control groups. But it something that evolve in time, when user adapting to change. If you see sudden change, then learning effect may not at fault.\n",
    "\n",
    "If all of the invariant metric has passed, then you can finally analyse the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Practical and Statistical Significance\n",
    "\n",
    "Next, for your evaluation metrics, calculate a confidence interval for the difference between the experiment and control groups, and check whether each metric is statistically and/or practically significance. A metric is statistically significant if the confidence interval does not include 0 (that is, you can be confident there was a change), and it is practically significant if the confidence interval does not include the practical significance boundary (that is, you can be confident there is a change that matters to the business.)\n",
    "If you have chosen multiple evaluation metrics, you will need to decide whether to use the Bonferroni correction. When deciding, keep in mind the results you are looking for in order to launch the experiment. Will the fact that you have multiple metrics make those results more likely to occur by chance than the alpha level of 0.05?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CI_left</th>\n",
       "      <th>CI_right</th>\n",
       "      <th>d</th>\n",
       "      <th>stat sig?</th>\n",
       "      <th>dmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CG</th>\n",
       "      <td>-0.0291202</td>\n",
       "      <td>-0.0119896</td>\n",
       "      <td>-0.0205549</td>\n",
       "      <td>yes</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN</th>\n",
       "      <td>-0.0116042</td>\n",
       "      <td>0.00185674</td>\n",
       "      <td>-0.00487372</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CI_left    CI_right           d stat sig?    dmin\n",
       "CG -0.0291202  -0.0119896  -0.0205549       yes   -0.01\n",
       "CN -0.0116042  0.00185674 -0.00487372        no  0.0075"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataframe test_results\n",
    "test_results = pd.DataFrame(columns=[\"CI_left\", \"CI_right\", \"d\",\"stat sig?\", \"dmin\"], index=[\"CG\", \"CN\"])\n",
    "\n",
    "#set alpha\n",
    "alpha = 0.05\n",
    "\n",
    "\n",
    "#run two proportion z test for both metrics\n",
    "for i,j in zip([\"Enrollments\", \"Payments\"],[\"CG\", \"CN\"]):\n",
    "    #compute sample conversion rates\n",
    "    conv_control = control_pd.iloc[:23][i].sum()/control_pd.iloc[:23][\"Clicks\"].sum()\n",
    "    conv_experiment = exp_pd.iloc[:23][i].sum()/exp_pd.iloc[:23][\"Clicks\"].sum()\n",
    "    \n",
    "    #compute observed difference between treatment and control conversion d\n",
    "    test_results.at[j, \"d\"] = conv_experiment-conv_control\n",
    "    \n",
    "    #compute sample standard deviations\n",
    "    S_control = (conv_control*(1-conv_control))**0.5\n",
    "    S_experiment = (conv_experiment*(1-conv_experiment))**0.5\n",
    "    \n",
    "    #compute SE_pooled\n",
    "    SE_pooled = (S_control**2/control_pd.iloc[:23][\"Clicks\"].sum()+S_experiment**2/exp_pd.iloc[:23][\"Clicks\"].sum())**0.5\n",
    "    \n",
    "    #compute 95% confidence interval around observed difference d\n",
    "    test_results.at[j, \"CI_left\"] = test_results.at[j, \"d\"]-(norm.ppf(1-alpha/2)*SE_pooled)\n",
    "    test_results.at[j, \"CI_right\"] = test_results.at[j, \"d\"]+(norm.ppf(1-alpha/2)*SE_pooled)\n",
    "      #check statistical significance\n",
    "    if test_results.at[j, \"CI_left\"] <= 0 <= test_results.at[j, \"CI_right\"]:\n",
    "        test_results.at[j, \"stat sig?\"] = \"no\"\n",
    "    else:\n",
    "        test_results.at[j, \"stat sig?\"] = \"yes\"\n",
    "    \n",
    "    #import dmin\n",
    "    test_results.at[j, \"dmin\"] = md.loc[j][\"dmin\"]\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Sign Tests\n",
    "\n",
    "For each evaluation metric, do a sign test using the day-by-day breakdown. If the sign test does not agree with the confidence interval for the difference, see if you can figure out why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Recommendation\n",
    "\n",
    "Finally, make a recommendation. Would you launch this experiment, not launch it, dig deeper, run a follow-up experiment, or is it a judgment call? If you would dig deeper, explain what area you would investigate. If you would run follow-up experiments, briefIy describe that experiment. If it is a judgment call, explain what factors would be relevant to the decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gross conversion: the observed gross conversion in the treatment group is around 2.06% smaller than the gross conversion observed in the control group. Further, we see that also the values within the confidence interval are most compatible with a negative effect. Lastly, this effect appears to be practically relevant as those values are smaller than dmin, the minimum effect size to be considered relevant for the business.\n",
    "\n",
    "Net conversion: While we cannot reject the null hypothesis for this test, we see that the observed net conversion in the treatment group is around 0.49% smaller than the net conversion observed in the control group. Further, the values that are considered most reasonabily compatible with the data range from -1.16% to 0.19%.\n",
    "\n",
    "Given these results, we can assume that the introduction of the \"Free Trial Screener\" may indeed help to set clearer expectations for students upfront. However, the results are less compatible with the assumption that the decrease in gross conversion is entirely absorbed by an improvement in the overall student experience and still less compatible with dmin(net conversion), the minimum effect size to be considered relevant for the business. Consequently, assuming that Udacity has a fair interest in increasing revenues, we would recommend to not roll out the \"Free Trial Screener\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
